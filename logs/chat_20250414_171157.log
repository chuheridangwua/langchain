2025-04-14 17:11:57,859 [INFO] src.utils.callbacks: 日志系统初始化完成
2025-04-14 17:11:57,860 [INFO] src.utils.callbacks: 日志文件路径: logs\chat_20250414_171157.log
2025-04-14 17:11:57,860 [INFO] src.utils.callbacks: Windows系统UTF-8编码设置完成
2025-04-14 17:11:57,862 [DEBUG] httpx: load_ssl_context verify=True cert=None trust_env=True http2=False
2025-04-14 17:11:57,863 [DEBUG] httpx: load_verify_locations cafile='y:\\langchain\\.conda\\Library\\ssl\\cacert.pem'
2025-04-14 17:11:58,087 [DEBUG] httpx: load_ssl_context verify=True cert=None trust_env=True http2=False
2025-04-14 17:11:58,088 [DEBUG] httpx: load_verify_locations cafile='y:\\langchain\\.conda\\Library\\ssl\\cacert.pem'
2025-04-14 17:12:03,082 [INFO] src.utils.callbacks: 初始化StreamingAgentCallbackHandler
2025-04-14 17:12:06,281 [DEBUG] src.chat: 聊天历史记录 (1 条消息):
2025-04-14 17:12:06,282 [DEBUG] src.chat: [0] human: 你好...
2025-04-14 17:12:06,290 [DEBUG] urllib3.connectionpool: Starting new HTTPS connection (1): api.smith.langchain.com:443
2025-04-14 17:12:06,602 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "你是一个智能助手，可以调用以下工具：\n\nget_current_time: 获取当前系统时间和日期\nsearch_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\ngenerate_and_execute_code: 根据自然语言指令生成并执行Python代码\nexecute_python_code: 执行指定的Python代码并返回结果\n\n\n下面是你与用户的对话历史，请记住这些信息，保持连贯性：\n\n[HumanMessage(content='你好', additional_kwargs={}, response_metadata={})]\n\n\n\n> get_current_time: 获取当前系统时间和日期\n> search_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\n> generate_and_execute_code: 根据自然语言指令生成并执行Python代码\n> execute_python_code: 执行指定的Python代码并返回结果\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [get_current_time, search_tool, generate_and_execute_code, execute_python_code]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nQuestion: 你好\n", 'role': 'user'}], 'model': 'deepseek-v3-250324', 'stop': ['\nObservation:', '\n\tObservation:'], 'stream': True, 'temperature': 0.7}}
2025-04-14 17:12:06,604 [DEBUG] openai._base_client: Sending HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions
2025-04-14 17:12:06,606 [DEBUG] httpcore.connection: connect_tcp.started host='ark.cn-beijing.volces.com' port=443 local_address=None timeout=None socket_options=None
2025-04-14 17:12:06,610 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A53FB27D0>
2025-04-14 17:12:06,611 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000028A3DB87BC0> server_hostname='ark.cn-beijing.volces.com' timeout=None
2025-04-14 17:12:06,652 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A53FB2530>
2025-04-14 17:12:06,653 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-04-14 17:12:06,654 [DEBUG] httpcore.http11: send_request_headers.complete
2025-04-14 17:12:06,654 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-04-14 17:12:06,655 [DEBUG] httpcore.http11: send_request_body.complete
2025-04-14 17:12:06,655 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-04-14 17:12:06,779 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "GET /info HTTP/1.1" 200 672
2025-04-14 17:12:06,847 [DEBUG] langsmith.client: Sending multipart request with context: trace=6f0fe4c5-d21f-4520-8714-e8f8d9aa7d48,id=6f0fe4c5-d21f-4520-8714-e8f8d9aa7d48; trace=6f0fe4c5-d21f-4520-8714-e8f8d9aa7d48,id=8dd33acd-ff7a-4e2b-a4c9-9ac7c237bae6; trace=6f0fe4c5-d21f-4520-8714-e8f8d9aa7d48,id=fe79b725-28b5-4a5d-b748-d92744c54351
2025-04-14 17:12:06,970 [DEBUG] urllib3.util.retry: Incremented Retry for (url='/runs/multipart'): LangSmithRetry(total=2, connect=None, read=None, redirect=None, status=None)
2025-04-14 17:12:06,970 [DEBUG] urllib3.connectionpool: Starting new HTTPS connection (2): api.smith.langchain.com:443
2025-04-14 17:12:07,076 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'server', b'istio-envoy'), (b'date', b'Mon, 14 Apr 2025 09:12:06 GMT'), (b'content-type', b'text/event-stream'), (b'x-client-request-id', b'unknown-20250414171206-nHLrdUrV'), (b'cache-control', b'no-cache'), (b'x-envoy-upstream-service-time', b'392'), (b'x-request-id', b'021744621926227079d8e2dbf97773446e607b2f9555f0e30071d'), (b'transfer-encoding', b'chunked')])
2025-04-14 17:12:07,077 [INFO] httpx: HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-04-14 17:12:07,077 [DEBUG] openai._base_client: HTTP Response: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "200 OK" Headers({'server': 'istio-envoy', 'date': 'Mon, 14 Apr 2025 09:12:06 GMT', 'content-type': 'text/event-stream', 'x-client-request-id': 'unknown-20250414171206-nHLrdUrV', 'cache-control': 'no-cache', 'x-envoy-upstream-service-time': '392', 'x-request-id': '021744621926227079d8e2dbf97773446e607b2f9555f0e30071d', 'transfer-encoding': 'chunked'})
2025-04-14 17:12:07,078 [DEBUG] openai._base_client: request_id: 021744621926227079d8e2dbf97773446e607b2f9555f0e30071d
2025-04-14 17:12:07,078 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-04-14 17:12:07,508 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:08,298 [DEBUG] httpcore.http11: receive_response_body.complete
2025-04-14 17:12:08,298 [DEBUG] httpcore.http11: response_closed.started
2025-04-14 17:12:08,298 [DEBUG] httpcore.http11: response_closed.complete
2025-04-14 17:12:08,301 [INFO] root: 已保存会话 session_20250414171203
2025-04-14 17:12:08,851 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=6f0fe4c5-d21f-4520-8714-e8f8d9aa7d48,id=fe79b725-28b5-4a5d-b748-d92744c54351; trace=6f0fe4c5-d21f-4520-8714-e8f8d9aa7d48,id=8dd33acd-ff7a-4e2b-a4c9-9ac7c237bae6; trace=6f0fe4c5-d21f-4520-8714-e8f8d9aa7d48,id=6f0fe4c5-d21f-4520-8714-e8f8d9aa7d48
2025-04-14 17:12:09,102 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:15,219 [DEBUG] src.chat: 聊天历史记录 (3 条消息):
2025-04-14 17:12:15,219 [DEBUG] src.chat: [0] human: 你好...
2025-04-14 17:12:15,219 [DEBUG] src.chat: [1] ai: 你好！我是你的智能助手，有什么可以帮你的吗？
```...
2025-04-14 17:12:15,220 [DEBUG] src.chat: [2] human: 我叫盛宏超...
2025-04-14 17:12:15,231 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "你是一个智能助手，可以调用以下工具：\n\nget_current_time: 获取当前系统时间和日期\nsearch_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\ngenerate_and_execute_code: 根据自然语言指令生成并执行Python代码\nexecute_python_code: 执行指定的Python代码并返回结果\n\n\n以下是关于当前用户的信息：\n用户信息记忆:\n用户名称: 盛宏超\n\n请记住这些信息并在回答时利用它们提供个性化的回应。\n\n下面是你与用户的对话历史，请记住这些信息，保持连贯性：\n\n[HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！我是你的智能助手，有什么可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我叫盛宏超', additional_kwargs={}, response_metadata={})]\n\n\n\n> get_current_time: 获取当前系统时间和日期\n> search_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\n> generate_and_execute_code: 根据自然语言指令生成并执行Python代码\n> execute_python_code: 执行指定的Python代码并返回结果\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [get_current_time, search_tool, generate_and_execute_code, execute_python_code]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nQuestion: 我叫盛宏超\n", 'role': 'user'}], 'model': 'deepseek-v3-250324', 'stop': ['\nObservation:', '\n\tObservation:'], 'stream': True, 'temperature': 0.7}}
2025-04-14 17:12:15,234 [DEBUG] openai._base_client: Sending HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions
2025-04-14 17:12:15,234 [DEBUG] httpcore.connection: close.started
2025-04-14 17:12:15,235 [DEBUG] httpcore.connection: close.complete
2025-04-14 17:12:15,235 [DEBUG] httpcore.connection: connect_tcp.started host='ark.cn-beijing.volces.com' port=443 local_address=None timeout=None socket_options=None
2025-04-14 17:12:15,237 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A3BEE35B0>
2025-04-14 17:12:15,237 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000028A3DB87BC0> server_hostname='ark.cn-beijing.volces.com' timeout=None
2025-04-14 17:12:15,294 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A3BEE3310>
2025-04-14 17:12:15,295 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-04-14 17:12:15,296 [DEBUG] httpcore.http11: send_request_headers.complete
2025-04-14 17:12:15,297 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-04-14 17:12:15,297 [DEBUG] httpcore.http11: send_request_body.complete
2025-04-14 17:12:15,298 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-04-14 17:12:15,585 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'server', b'istio-envoy'), (b'date', b'Mon, 14 Apr 2025 09:12:14 GMT'), (b'content-type', b'text/event-stream'), (b'x-client-request-id', b'unknown-20250414171214-INiAONvM'), (b'cache-control', b'no-cache'), (b'x-envoy-upstream-service-time', b'269'), (b'x-request-id', b'021744621934861642231612c26693882d69e653a7ceebf2ce7ad'), (b'transfer-encoding', b'chunked')])
2025-04-14 17:12:15,586 [INFO] httpx: HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-04-14 17:12:15,586 [DEBUG] openai._base_client: HTTP Response: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "200 OK" Headers({'server': 'istio-envoy', 'date': 'Mon, 14 Apr 2025 09:12:14 GMT', 'content-type': 'text/event-stream', 'x-client-request-id': 'unknown-20250414171214-INiAONvM', 'cache-control': 'no-cache', 'x-envoy-upstream-service-time': '269', 'x-request-id': '021744621934861642231612c26693882d69e653a7ceebf2ce7ad', 'transfer-encoding': 'chunked'})
2025-04-14 17:12:15,587 [DEBUG] openai._base_client: request_id: 021744621934861642231612c26693882d69e653a7ceebf2ce7ad
2025-04-14 17:12:15,587 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-04-14 17:12:15,776 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=372fead4-b041-40ba-8af9-86fc31114d67,id=372fead4-b041-40ba-8af9-86fc31114d67; trace=372fead4-b041-40ba-8af9-86fc31114d67,id=5cfb7300-8107-49a0-b099-f92375bc36eb; trace=372fead4-b041-40ba-8af9-86fc31114d67,id=6e667d56-81d0-477d-9987-e1a0eeb57a02
2025-04-14 17:12:16,022 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:17,370 [DEBUG] httpcore.http11: receive_response_body.complete
2025-04-14 17:12:17,371 [DEBUG] httpcore.http11: response_closed.started
2025-04-14 17:12:17,371 [DEBUG] httpcore.http11: response_closed.complete
2025-04-14 17:12:17,376 [INFO] root: 已保存会话 session_20250414171203
2025-04-14 17:12:17,869 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=372fead4-b041-40ba-8af9-86fc31114d67,id=6e667d56-81d0-477d-9987-e1a0eeb57a02; trace=372fead4-b041-40ba-8af9-86fc31114d67,id=5cfb7300-8107-49a0-b099-f92375bc36eb; trace=372fead4-b041-40ba-8af9-86fc31114d67,id=372fead4-b041-40ba-8af9-86fc31114d67
2025-04-14 17:12:18,114 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:25,311 [DEBUG] src.chat: 聊天历史记录 (5 条消息):
2025-04-14 17:12:25,312 [DEBUG] src.chat: [0] human: 你好...
2025-04-14 17:12:25,312 [DEBUG] src.chat: [1] ai: 你好！我是你的智能助手，有什么可以帮你的吗？
```...
2025-04-14 17:12:25,313 [DEBUG] src.chat: [2] human: 我叫盛宏超...
2025-04-14 17:12:25,313 [DEBUG] src.chat: [3] ai: 你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗...
2025-04-14 17:12:25,313 [DEBUG] src.chat: [4] human: 我是谁...
2025-04-14 17:12:25,320 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "你是一个智能助手，可以调用以下工具：\n\nget_current_time: 获取当前系统时间和日期\nsearch_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\ngenerate_and_execute_code: 根据自然语言指令生成并执行Python代码\nexecute_python_code: 执行指定的Python代码并返回结果\n\n\n以下是关于当前用户的信息：\n用户信息记忆:\n用户名称: 盛宏超\n\n请记住这些信息并在回答时利用它们提供个性化的回应。\n\n下面是你与用户的对话历史，请记住这些信息，保持连贯性：\n\n[HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！我是你的智能助手，有什么可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我叫盛宏超', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我是谁', additional_kwargs={}, response_metadata={})]\n\n\n\n> get_current_time: 获取当前系统时间和日期\n> search_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\n> generate_and_execute_code: 根据自然语言指令生成并执行Python代码\n> execute_python_code: 执行指定的Python代码并返回结果\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [get_current_time, search_tool, generate_and_execute_code, execute_python_code]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nQuestion: 我是谁\n", 'role': 'user'}], 'model': 'deepseek-v3-250324', 'stop': ['\nObservation:', '\n\tObservation:'], 'stream': True, 'temperature': 0.7}}
2025-04-14 17:12:25,324 [DEBUG] openai._base_client: Sending HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions
2025-04-14 17:12:25,325 [DEBUG] httpcore.connection: close.started
2025-04-14 17:12:25,325 [DEBUG] httpcore.connection: close.complete
2025-04-14 17:12:25,325 [DEBUG] httpcore.connection: connect_tcp.started host='ark.cn-beijing.volces.com' port=443 local_address=None timeout=None socket_options=None
2025-04-14 17:12:25,378 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A3D7AD180>
2025-04-14 17:12:25,379 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000028A3DB87BC0> server_hostname='ark.cn-beijing.volces.com' timeout=None
2025-04-14 17:12:25,411 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A3D7ACEE0>
2025-04-14 17:12:25,412 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-04-14 17:12:25,413 [DEBUG] httpcore.http11: send_request_headers.complete
2025-04-14 17:12:25,413 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-04-14 17:12:25,413 [DEBUG] httpcore.http11: send_request_body.complete
2025-04-14 17:12:25,413 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-04-14 17:12:25,873 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=03959e4b-e3c8-4205-aece-4c1750e09b43,id=03959e4b-e3c8-4205-aece-4c1750e09b43; trace=03959e4b-e3c8-4205-aece-4c1750e09b43,id=b0414c9c-eae7-4ae9-9ce1-f4f3ba20ef83; trace=03959e4b-e3c8-4205-aece-4c1750e09b43,id=88617067-3d6d-4bac-83f9-5df05ba4a479
2025-04-14 17:12:25,904 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'server', b'istio-envoy'), (b'date', b'Mon, 14 Apr 2025 09:12:24 GMT'), (b'content-type', b'text/event-stream'), (b'x-client-request-id', b'unknown-20250414171224-oWUmbmRC'), (b'cache-control', b'no-cache'), (b'x-envoy-upstream-service-time', b'474'), (b'x-request-id', b'021744621944975186b1bbde65e9586a669db9f3bf76b1070a580'), (b'transfer-encoding', b'chunked')])
2025-04-14 17:12:25,906 [INFO] httpx: HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-04-14 17:12:25,906 [DEBUG] openai._base_client: HTTP Response: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "200 OK" Headers({'server': 'istio-envoy', 'date': 'Mon, 14 Apr 2025 09:12:24 GMT', 'content-type': 'text/event-stream', 'x-client-request-id': 'unknown-20250414171224-oWUmbmRC', 'cache-control': 'no-cache', 'x-envoy-upstream-service-time': '474', 'x-request-id': '021744621944975186b1bbde65e9586a669db9f3bf76b1070a580', 'transfer-encoding': 'chunked'})
2025-04-14 17:12:25,906 [DEBUG] openai._base_client: request_id: 021744621944975186b1bbde65e9586a669db9f3bf76b1070a580
2025-04-14 17:12:25,907 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-04-14 17:12:26,116 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:27,179 [DEBUG] httpcore.http11: receive_response_body.complete
2025-04-14 17:12:27,180 [DEBUG] httpcore.http11: response_closed.started
2025-04-14 17:12:27,180 [DEBUG] httpcore.http11: response_closed.complete
2025-04-14 17:12:27,182 [INFO] root: 已保存会话 session_20250414171203
2025-04-14 17:12:27,733 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=03959e4b-e3c8-4205-aece-4c1750e09b43,id=88617067-3d6d-4bac-83f9-5df05ba4a479; trace=03959e4b-e3c8-4205-aece-4c1750e09b43,id=b0414c9c-eae7-4ae9-9ce1-f4f3ba20ef83; trace=03959e4b-e3c8-4205-aece-4c1750e09b43,id=03959e4b-e3c8-4205-aece-4c1750e09b43
2025-04-14 17:12:27,974 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:49,488 [DEBUG] src.chat: 聊天历史记录 (7 条消息):
2025-04-14 17:12:49,488 [DEBUG] src.chat: [0] human: 你好...
2025-04-14 17:12:49,489 [DEBUG] src.chat: [1] ai: 你好！我是你的智能助手，有什么可以帮你的吗？
```...
2025-04-14 17:12:49,489 [DEBUG] src.chat: [2] human: 我叫盛宏超...
2025-04-14 17:12:49,489 [DEBUG] src.chat: [3] ai: 你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗...
2025-04-14 17:12:49,489 [DEBUG] src.chat: [4] human: 我是谁...
2025-04-14 17:12:49,489 [DEBUG] src.chat: [5] ai: 你是盛宏超啊！我之前已经记住了你的名字。有什么需要我帮忙的吗？
```...
2025-04-14 17:12:49,490 [DEBUG] src.chat: [6] human: 查一下今天的热点新闻吧...
2025-04-14 17:12:49,496 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "你是一个智能助手，可以调用以下工具：\n\nget_current_time: 获取当前系统时间和日期\nsearch_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\ngenerate_and_execute_code: 根据自然语言指令生成并执行Python代码\nexecute_python_code: 执行指定的Python代码并返回结果\n\n\n以下是关于当前用户的信息：\n用户信息记忆:\n用户名称: 盛宏超\n\n请记住这些信息并在回答时利用它们提供个性化的回应。\n\n下面是你与用户的对话历史，请记住这些信息，保持连贯性：\n\n[HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！我是你的智能助手，有什么可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我叫盛宏超', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我是谁', additional_kwargs={}, response_metadata={}), AIMessage(content='你是盛宏超啊！我之前已经记住了你的名字。有什么需要我帮忙的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='查一下今天的热点新闻吧', additional_kwargs={}, response_metadata={})]\n\n\n\n> get_current_time: 获取当前系统时间和日期\n> search_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\n> generate_and_execute_code: 根据自然语言指令生成并执行Python代码\n> execute_python_code: 执行指定的Python代码并返回结果\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [get_current_time, search_tool, generate_and_execute_code, execute_python_code]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nQuestion: 查一下今天的热点新闻吧\n", 'role': 'user'}], 'model': 'deepseek-v3-250324', 'stop': ['\nObservation:', '\n\tObservation:'], 'stream': True, 'temperature': 0.7}}
2025-04-14 17:12:49,499 [DEBUG] openai._base_client: Sending HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions
2025-04-14 17:12:49,499 [DEBUG] httpcore.connection: close.started
2025-04-14 17:12:49,499 [DEBUG] httpcore.connection: close.complete
2025-04-14 17:12:49,500 [DEBUG] httpcore.connection: connect_tcp.started host='ark.cn-beijing.volces.com' port=443 local_address=None timeout=None socket_options=None
2025-04-14 17:12:49,502 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A3D7AF010>
2025-04-14 17:12:49,502 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000028A3DB87BC0> server_hostname='ark.cn-beijing.volces.com' timeout=None
2025-04-14 17:12:49,543 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A3D7AED70>
2025-04-14 17:12:49,544 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-04-14 17:12:49,545 [DEBUG] httpcore.http11: send_request_headers.complete
2025-04-14 17:12:49,545 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-04-14 17:12:49,546 [DEBUG] httpcore.http11: send_request_body.complete
2025-04-14 17:12:49,546 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-04-14 17:12:49,961 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'server', b'istio-envoy'), (b'date', b'Mon, 14 Apr 2025 09:12:48 GMT'), (b'content-type', b'text/event-stream'), (b'x-client-request-id', b'unknown-20250414171249-ZYGLMgjf'), (b'cache-control', b'no-cache'), (b'x-envoy-upstream-service-time', b'395'), (b'x-request-id', b'021744621969111384cfaa31ec6325bb0f7a8325e5c6fc58a8d1b'), (b'transfer-encoding', b'chunked')])
2025-04-14 17:12:49,962 [INFO] httpx: HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-04-14 17:12:49,962 [DEBUG] openai._base_client: HTTP Response: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "200 OK" Headers({'server': 'istio-envoy', 'date': 'Mon, 14 Apr 2025 09:12:48 GMT', 'content-type': 'text/event-stream', 'x-client-request-id': 'unknown-20250414171249-ZYGLMgjf', 'cache-control': 'no-cache', 'x-envoy-upstream-service-time': '395', 'x-request-id': '021744621969111384cfaa31ec6325bb0f7a8325e5c6fc58a8d1b', 'transfer-encoding': 'chunked'})
2025-04-14 17:12:49,962 [DEBUG] openai._base_client: request_id: 021744621969111384cfaa31ec6325bb0f7a8325e5c6fc58a8d1b
2025-04-14 17:12:49,963 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-04-14 17:12:50,044 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=f9a82b35-8191-4414-8a32-683a609f95d5; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=7cb1edcd-bb06-4da9-8257-6f3aca1709a9; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=efe9eba2-a68f-44b4-85c5-5fbe34ffcb27
2025-04-14 17:12:50,304 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:51,611 [DEBUG] httpcore.http11: receive_response_body.complete
2025-04-14 17:12:51,611 [DEBUG] httpcore.http11: response_closed.started
2025-04-14 17:12:51,612 [DEBUG] httpcore.http11: response_closed.complete
2025-04-14 17:12:51,617 [DEBUG] urllib3.connectionpool: Starting new HTTPS connection (1): api.tavily.com:443
2025-04-14 17:12:52,171 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=efe9eba2-a68f-44b4-85c5-5fbe34ffcb27; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=7cb1edcd-bb06-4da9-8257-6f3aca1709a9; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=aadbaf4d-541a-453c-ae6a-6881de8347bc; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=202fdd5c-cb39-4f45-8fb2-bc7079cbbd5b
2025-04-14 17:12:52,417 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:54,199 [DEBUG] urllib3.connectionpool: https://api.tavily.com:443 "POST /search HTTP/1.1" 200 145
2025-04-14 17:12:54,208 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': '你是一个智能助手，可以调用以下工具：\n\nget_current_time: 获取当前系统时间和日期\nsearch_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\ngenerate_and_execute_code: 根据自然语言指令生成并执行Python代码\nexecute_python_code: 执行指定的Python代码并返回结果\n\n\n以下是关于当前用户的信息：\n用户信息记忆:\n用户名称: 盛宏超\n\n请记住这些信息并在回答时利用它们提供个性化的回应。\n\n下面是你与用户的对话历史，请记住这些信息，保持连贯性：\n\n[HumanMessage(content=\'你好\', additional_kwargs={}, response_metadata={}), AIMessage(content=\'你好！我是你的智能助手，有什么可以帮你的吗？\\n```\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\'我叫盛宏超\', additional_kwargs={}, response_metadata={}), AIMessage(content=\'你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗？\\n```\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\'我是谁\', additional_kwargs={}, response_metadata={}), AIMessage(content=\'你是盛宏超啊！我之前已经记住了你的名字。有什么需要我帮忙的吗？\\n```\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\'查一下今天的热点新闻吧\', additional_kwargs={}, response_metadata={})]\n\n\n\n> get_current_time: 获取当前系统时间和日期\n> search_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\n> generate_and_execute_code: 根据自然语言指令生成并执行Python代码\n> execute_python_code: 执行指定的Python代码并返回结果\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [get_current_time, search_tool, generate_and_execute_code, execute_python_code]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nQuestion: 查一下今天的热点新闻吧\n```\nThought: Do I need to use a tool? Yes\nAction: search_tool\nAction Input: {"query": "2023年11月3日 热点新闻"}\nObservation: []\nThought:', 'role': 'user'}], 'model': 'deepseek-v3-250324', 'stop': ['\nObservation:', '\n\tObservation:'], 'stream': True, 'temperature': 0.7}}
2025-04-14 17:12:54,210 [DEBUG] openai._base_client: Sending HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions
2025-04-14 17:12:54,210 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-04-14 17:12:54,211 [DEBUG] httpcore.http11: send_request_headers.complete
2025-04-14 17:12:54,211 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-04-14 17:12:54,211 [DEBUG] httpcore.http11: send_request_body.complete
2025-04-14 17:12:54,212 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-04-14 17:12:54,503 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'server', b'istio-envoy'), (b'date', b'Mon, 14 Apr 2025 09:12:54 GMT'), (b'content-type', b'text/event-stream'), (b'x-client-request-id', b'unknown-20250414171253-NWSjFiTd'), (b'cache-control', b'no-cache'), (b'x-envoy-upstream-service-time', b'271'), (b'x-request-id', b'021744621973777384cfaa31ec6325bb0f7a8325e5c6fc56c216f'), (b'transfer-encoding', b'chunked')])
2025-04-14 17:12:54,504 [INFO] httpx: HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-04-14 17:12:54,504 [DEBUG] openai._base_client: HTTP Response: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "200 OK" Headers({'server': 'istio-envoy', 'date': 'Mon, 14 Apr 2025 09:12:54 GMT', 'content-type': 'text/event-stream', 'x-client-request-id': 'unknown-20250414171253-NWSjFiTd', 'cache-control': 'no-cache', 'x-envoy-upstream-service-time': '271', 'x-request-id': '021744621973777384cfaa31ec6325bb0f7a8325e5c6fc56c216f', 'transfer-encoding': 'chunked'})
2025-04-14 17:12:54,505 [DEBUG] openai._base_client: request_id: 021744621973777384cfaa31ec6325bb0f7a8325e5c6fc56c216f
2025-04-14 17:12:54,505 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-04-14 17:12:54,764 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=202fdd5c-cb39-4f45-8fb2-bc7079cbbd5b; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=aadbaf4d-541a-453c-ae6a-6881de8347bc; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=60c0a67d-dc61-4513-8c0a-4aacbf08bcb8; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=34f14fcc-f139-4cd0-8a97-cc5e7a3e1c92
2025-04-14 17:12:55,009 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:55,503 [DEBUG] httpcore.http11: receive_response_body.complete
2025-04-14 17:12:55,503 [DEBUG] httpcore.http11: response_closed.started
2025-04-14 17:12:55,503 [DEBUG] httpcore.http11: response_closed.complete
2025-04-14 17:12:55,513 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': '你是一个智能助手，可以调用以下工具：\n\nget_current_time: 获取当前系统时间和日期\nsearch_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\ngenerate_and_execute_code: 根据自然语言指令生成并执行Python代码\nexecute_python_code: 执行指定的Python代码并返回结果\n\n\n以下是关于当前用户的信息：\n用户信息记忆:\n用户名称: 盛宏超\n\n请记住这些信息并在回答时利用它们提供个性化的回应。\n\n下面是你与用户的对话历史，请记住这些信息，保持连贯性：\n\n[HumanMessage(content=\'你好\', additional_kwargs={}, response_metadata={}), AIMessage(content=\'你好！我是你的智能助手，有什么可以帮你的吗？\\n```\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\'我叫盛宏超\', additional_kwargs={}, response_metadata={}), AIMessage(content=\'你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗？\\n```\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\'我是谁\', additional_kwargs={}, response_metadata={}), AIMessage(content=\'你是盛宏超啊！我之前已经记住了你的名字。有什么需要我帮忙的吗？\\n```\', additional_kwargs={}, response_metadata={}), HumanMessage(content=\'查一下今天的热点新闻吧\', additional_kwargs={}, response_metadata={})]\n\n\n\n> get_current_time: 获取当前系统时间和日期\n> search_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\n> generate_and_execute_code: 根据自然语言指令生成并执行Python代码\n> execute_python_code: 执行指定的Python代码并返回结果\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [get_current_time, search_tool, generate_and_execute_code, execute_python_code]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nQuestion: 查一下今天的热点新闻吧\n```\nThought: Do I need to use a tool? Yes\nAction: search_tool\nAction Input: {"query": "2023年11月3日 热点新闻"}\nObservation: []\nThought:```\nThought: Do I need to use a tool? Yes\nAction: get_current_time\nAction Input: {}\nObservation: 当前时间是: 2025年04月14日 17:12:55\nThought:', 'role': 'user'}], 'model': 'deepseek-v3-250324', 'stop': ['\nObservation:', '\n\tObservation:'], 'stream': True, 'temperature': 0.7}}
2025-04-14 17:12:55,514 [DEBUG] openai._base_client: Sending HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions
2025-04-14 17:12:55,514 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-04-14 17:12:55,515 [DEBUG] httpcore.http11: send_request_headers.complete
2025-04-14 17:12:55,516 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-04-14 17:12:55,516 [DEBUG] httpcore.http11: send_request_body.complete
2025-04-14 17:12:55,516 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-04-14 17:12:56,066 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=34f14fcc-f139-4cd0-8a97-cc5e7a3e1c92; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=60c0a67d-dc61-4513-8c0a-4aacbf08bcb8; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=8d0f4b26-8716-40ff-afe4-9305f120cb9c; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=8d0f4b26-8716-40ff-afe4-9305f120cb9c; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=fb869d70-3973-4b11-9ba7-7c2a1cc1ed39; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=710c5bf5-0f52-454b-b6b0-e9de98caeb89
2025-04-14 17:12:56,169 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'server', b'istio-envoy'), (b'date', b'Mon, 14 Apr 2025 09:12:54 GMT'), (b'content-type', b'text/event-stream'), (b'x-client-request-id', b'unknown-20250414171255-clcMGHSE'), (b'cache-control', b'no-cache'), (b'x-envoy-upstream-service-time', b'634'), (b'x-request-id', b'021744621975081384cfaa31ec6325bb0f7a8325e5c6fc51866a6'), (b'transfer-encoding', b'chunked')])
2025-04-14 17:12:56,171 [INFO] httpx: HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-04-14 17:12:56,171 [DEBUG] openai._base_client: HTTP Response: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "200 OK" Headers({'server': 'istio-envoy', 'date': 'Mon, 14 Apr 2025 09:12:54 GMT', 'content-type': 'text/event-stream', 'x-client-request-id': 'unknown-20250414171255-clcMGHSE', 'cache-control': 'no-cache', 'x-envoy-upstream-service-time': '634', 'x-request-id': '021744621975081384cfaa31ec6325bb0f7a8325e5c6fc51866a6', 'transfer-encoding': 'chunked'})
2025-04-14 17:12:56,171 [DEBUG] openai._base_client: request_id: 021744621975081384cfaa31ec6325bb0f7a8325e5c6fc51866a6
2025-04-14 17:12:56,172 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-04-14 17:12:56,309 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:12:57,675 [DEBUG] httpcore.http11: receive_response_body.complete
2025-04-14 17:12:57,675 [DEBUG] httpcore.http11: response_closed.started
2025-04-14 17:12:57,676 [DEBUG] httpcore.http11: response_closed.complete
2025-04-14 17:12:57,681 [DEBUG] urllib3.connectionpool: Starting new HTTPS connection (1): api.tavily.com:443
2025-04-14 17:12:58,234 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=710c5bf5-0f52-454b-b6b0-e9de98caeb89; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=fb869d70-3973-4b11-9ba7-7c2a1cc1ed39; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=08dfafc1-bfc3-4579-b19c-fe286a11af6b; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=d16382a2-33f2-45c0-bb34-f7973990b6f1
2025-04-14 17:12:58,479 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:13:00,000 [DEBUG] urllib3.connectionpool: https://api.tavily.com:443 "POST /search HTTP/1.1" 200 145
2025-04-14 17:13:00,003 [INFO] root: 已保存会话 session_20250414171203
2025-04-14 17:13:00,498 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=d16382a2-33f2-45c0-bb34-f7973990b6f1; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=08dfafc1-bfc3-4579-b19c-fe286a11af6b; trace=f9a82b35-8191-4414-8a32-683a609f95d5,id=f9a82b35-8191-4414-8a32-683a609f95d5
2025-04-14 17:13:00,740 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:13:17,690 [DEBUG] src.chat: 聊天历史记录 (9 条消息):
2025-04-14 17:13:17,690 [DEBUG] src.chat: [0] human: 你好...
2025-04-14 17:13:17,690 [DEBUG] src.chat: [1] ai: 你好！我是你的智能助手，有什么可以帮你的吗？
```...
2025-04-14 17:13:17,690 [DEBUG] src.chat: [2] human: 我叫盛宏超...
2025-04-14 17:13:17,690 [DEBUG] src.chat: [3] ai: 你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗...
2025-04-14 17:13:17,691 [DEBUG] src.chat: [4] human: 我是谁...
2025-04-14 17:13:17,691 [DEBUG] src.chat: [5] ai: 你是盛宏超啊！我之前已经记住了你的名字。有什么需要我帮忙的吗？
```...
2025-04-14 17:13:17,691 [DEBUG] src.chat: [6] human: 查一下今天的热点新闻吧...
2025-04-14 17:13:17,691 [DEBUG] src.chat: [7] ai: Agent stopped due to iteration limit or time limit...
2025-04-14 17:13:17,691 [DEBUG] src.chat: [8] human: 今天周几...
2025-04-14 17:13:17,702 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "你是一个智能助手，可以调用以下工具：\n\nget_current_time: 获取当前系统时间和日期\nsearch_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\ngenerate_and_execute_code: 根据自然语言指令生成并执行Python代码\nexecute_python_code: 执行指定的Python代码并返回结果\n\n\n以下是关于当前用户的信息：\n用户信息记忆:\n用户名称: 盛宏超\n\n请记住这些信息并在回答时利用它们提供个性化的回应。\n\n下面是你与用户的对话历史，请记住这些信息，保持连贯性：\n\n[HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！我是你的智能助手，有什么可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我叫盛宏超', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我是谁', additional_kwargs={}, response_metadata={}), AIMessage(content='你是盛宏超啊！我之前已经记住了你的名字。有什么需要我帮忙的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='查一下今天的热点新闻吧', additional_kwargs={}, response_metadata={}), AIMessage(content='Agent stopped due to iteration limit or time limit.', additional_kwargs={}, response_metadata={}), HumanMessage(content='今天周几', additional_kwargs={}, response_metadata={})]\n\n\n\n> get_current_time: 获取当前系统时间和日期\n> search_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\n> generate_and_execute_code: 根据自然语言指令生成并执行Python代码\n> execute_python_code: 执行指定的Python代码并返回结果\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [get_current_time, search_tool, generate_and_execute_code, execute_python_code]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nQuestion: 今天周几\n", 'role': 'user'}], 'model': 'deepseek-v3-250324', 'stop': ['\nObservation:', '\n\tObservation:'], 'stream': True, 'temperature': 0.7}}
2025-04-14 17:13:17,704 [DEBUG] openai._base_client: Sending HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions
2025-04-14 17:13:17,704 [DEBUG] httpcore.connection: close.started
2025-04-14 17:13:17,705 [DEBUG] httpcore.connection: close.complete
2025-04-14 17:13:17,705 [DEBUG] httpcore.connection: connect_tcp.started host='ark.cn-beijing.volces.com' port=443 local_address=None timeout=None socket_options=None
2025-04-14 17:13:17,706 [DEBUG] httpcore.connection: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A39234670>
2025-04-14 17:13:17,707 [DEBUG] httpcore.connection: start_tls.started ssl_context=<ssl.SSLContext object at 0x0000028A3DB87BC0> server_hostname='ark.cn-beijing.volces.com' timeout=None
2025-04-14 17:13:17,759 [DEBUG] httpcore.connection: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000028A39234700>
2025-04-14 17:13:17,759 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-04-14 17:13:17,760 [DEBUG] httpcore.http11: send_request_headers.complete
2025-04-14 17:13:17,761 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-04-14 17:13:17,761 [DEBUG] httpcore.http11: send_request_body.complete
2025-04-14 17:13:17,761 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-04-14 17:13:18,198 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=0035a372-ac66-4264-a773-861a907b8b29,id=0035a372-ac66-4264-a773-861a907b8b29; trace=0035a372-ac66-4264-a773-861a907b8b29,id=465288be-0b18-4d56-8046-2e0006c97dff; trace=0035a372-ac66-4264-a773-861a907b8b29,id=2164b74f-167e-4c93-b403-fac2322510a7
2025-04-14 17:13:18,445 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:13:18,501 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'server', b'istio-envoy'), (b'date', b'Mon, 14 Apr 2025 09:13:17 GMT'), (b'content-type', b'text/event-stream'), (b'x-client-request-id', b'unknown-20250414171317-CdmpoESD'), (b'cache-control', b'no-cache'), (b'x-envoy-upstream-service-time', b'719'), (b'x-request-id', b'02174462199732615208d1b4038f629a958d0e327ef7d33f8eaa7'), (b'transfer-encoding', b'chunked')])
2025-04-14 17:13:18,502 [INFO] httpx: HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-04-14 17:13:18,502 [DEBUG] openai._base_client: HTTP Response: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "200 OK" Headers({'server': 'istio-envoy', 'date': 'Mon, 14 Apr 2025 09:13:17 GMT', 'content-type': 'text/event-stream', 'x-client-request-id': 'unknown-20250414171317-CdmpoESD', 'cache-control': 'no-cache', 'x-envoy-upstream-service-time': '719', 'x-request-id': '02174462199732615208d1b4038f629a958d0e327ef7d33f8eaa7', 'transfer-encoding': 'chunked'})
2025-04-14 17:13:18,503 [DEBUG] openai._base_client: request_id: 02174462199732615208d1b4038f629a958d0e327ef7d33f8eaa7
2025-04-14 17:13:18,503 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-04-14 17:13:19,911 [DEBUG] httpcore.http11: receive_response_body.complete
2025-04-14 17:13:19,912 [DEBUG] httpcore.http11: response_closed.started
2025-04-14 17:13:19,912 [DEBUG] httpcore.http11: response_closed.complete
2025-04-14 17:13:19,920 [DEBUG] openai._base_client: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': "你是一个智能助手，可以调用以下工具：\n\nget_current_time: 获取当前系统时间和日期\nsearch_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\ngenerate_and_execute_code: 根据自然语言指令生成并执行Python代码\nexecute_python_code: 执行指定的Python代码并返回结果\n\n\n以下是关于当前用户的信息：\n用户信息记忆:\n用户名称: 盛宏超\n\n请记住这些信息并在回答时利用它们提供个性化的回应。\n\n下面是你与用户的对话历史，请记住这些信息，保持连贯性：\n\n[HumanMessage(content='你好', additional_kwargs={}, response_metadata={}), AIMessage(content='你好！我是你的智能助手，有什么可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我叫盛宏超', additional_kwargs={}, response_metadata={}), AIMessage(content='你好，盛宏超！很高兴认识你。我已经记住了你的名字，以后会为你提供更个性化的服务。有什么我可以帮你的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='我是谁', additional_kwargs={}, response_metadata={}), AIMessage(content='你是盛宏超啊！我之前已经记住了你的名字。有什么需要我帮忙的吗？\\n```', additional_kwargs={}, response_metadata={}), HumanMessage(content='查一下今天的热点新闻吧', additional_kwargs={}, response_metadata={}), AIMessage(content='Agent stopped due to iteration limit or time limit.', additional_kwargs={}, response_metadata={}), HumanMessage(content='今天周几', additional_kwargs={}, response_metadata={})]\n\n\n\n> get_current_time: 获取当前系统时间和日期\n> search_tool: 使用Tavily进行网络搜索，返回最相关的3条搜索结果，每条结果包含标题、URL和摘要信息。\n> generate_and_execute_code: 根据自然语言指令生成并执行Python代码\n> execute_python_code: 执行指定的Python代码并返回结果\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [get_current_time, search_tool, generate_and_execute_code, execute_python_code]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nAI: [your response here]\n```\n\nQuestion: 今天周几\n```\nThought: Do I need to use a tool? Yes\nAction: get_current_time\nAction Input: {}\nObservation: 当前时间是: 2025年04月14日 17:13:19\nThought:", 'role': 'user'}], 'model': 'deepseek-v3-250324', 'stop': ['\nObservation:', '\n\tObservation:'], 'stream': True, 'temperature': 0.7}}
2025-04-14 17:13:19,923 [DEBUG] openai._base_client: Sending HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions
2025-04-14 17:13:19,923 [DEBUG] httpcore.http11: send_request_headers.started request=<Request [b'POST']>
2025-04-14 17:13:19,924 [DEBUG] httpcore.http11: send_request_headers.complete
2025-04-14 17:13:19,925 [DEBUG] httpcore.http11: send_request_body.started request=<Request [b'POST']>
2025-04-14 17:13:19,925 [DEBUG] httpcore.http11: send_request_body.complete
2025-04-14 17:13:19,925 [DEBUG] httpcore.http11: receive_response_headers.started request=<Request [b'POST']>
2025-04-14 17:13:20,220 [DEBUG] httpcore.http11: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'server', b'istio-envoy'), (b'date', b'Mon, 14 Apr 2025 09:13:19 GMT'), (b'content-type', b'text/event-stream'), (b'x-client-request-id', b'unknown-20250414171319-TzRmBnye'), (b'cache-control', b'no-cache'), (b'x-envoy-upstream-service-time', b'274'), (b'x-request-id', b'02174462199949015208d1b4038f629a958d0e327ef7d33cb2364'), (b'transfer-encoding', b'chunked')])
2025-04-14 17:13:20,221 [INFO] httpx: HTTP Request: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "HTTP/1.1 200 OK"
2025-04-14 17:13:20,221 [DEBUG] openai._base_client: HTTP Response: POST https://ark.cn-beijing.volces.com/api/v3/chat/completions "200 OK" Headers({'server': 'istio-envoy', 'date': 'Mon, 14 Apr 2025 09:13:19 GMT', 'content-type': 'text/event-stream', 'x-client-request-id': 'unknown-20250414171319-TzRmBnye', 'cache-control': 'no-cache', 'x-envoy-upstream-service-time': '274', 'x-request-id': '02174462199949015208d1b4038f629a958d0e327ef7d33cb2364', 'transfer-encoding': 'chunked'})
2025-04-14 17:13:20,221 [DEBUG] openai._base_client: request_id: 02174462199949015208d1b4038f629a958d0e327ef7d33cb2364
2025-04-14 17:13:20,222 [DEBUG] httpcore.http11: receive_response_body.started request=<Request [b'POST']>
2025-04-14 17:13:20,412 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=0035a372-ac66-4264-a773-861a907b8b29,id=2164b74f-167e-4c93-b403-fac2322510a7; trace=0035a372-ac66-4264-a773-861a907b8b29,id=465288be-0b18-4d56-8046-2e0006c97dff; trace=0035a372-ac66-4264-a773-861a907b8b29,id=0fd27c32-97f0-4245-b883-63e2647e8737; trace=0035a372-ac66-4264-a773-861a907b8b29,id=0fd27c32-97f0-4245-b883-63e2647e8737; trace=0035a372-ac66-4264-a773-861a907b8b29,id=3ecca7a0-ab41-479e-84d6-767e52685ce3; trace=0035a372-ac66-4264-a773-861a907b8b29,id=5720a01e-f1b2-4710-b6a6-1caa17dd448c
2025-04-14 17:13:20,657 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:13:22,059 [DEBUG] httpcore.http11: receive_response_body.complete
2025-04-14 17:13:22,059 [DEBUG] httpcore.http11: response_closed.started
2025-04-14 17:13:22,060 [DEBUG] httpcore.http11: response_closed.complete
2025-04-14 17:13:22,062 [INFO] root: 已保存会话 session_20250414171203
2025-04-14 17:13:22,614 [DEBUG] langsmith.client: Sending compressed multipart request with context: trace=0035a372-ac66-4264-a773-861a907b8b29,id=5720a01e-f1b2-4710-b6a6-1caa17dd448c; trace=0035a372-ac66-4264-a773-861a907b8b29,id=3ecca7a0-ab41-479e-84d6-767e52685ce3; trace=0035a372-ac66-4264-a773-861a907b8b29,id=0035a372-ac66-4264-a773-861a907b8b29
2025-04-14 17:13:22,858 [DEBUG] urllib3.connectionpool: https://api.smith.langchain.com:443 "POST /runs/multipart HTTP/1.1" 202 34
2025-04-14 17:13:32,548 [INFO] root: 已保存会话 session_20250414171203
2025-04-14 17:13:32,584 [DEBUG] langsmith.client: Compressed traces control thread is shutting down
2025-04-14 17:13:32,585 [DEBUG] langsmith.client: Closing Client.session
2025-04-14 17:13:32,586 [DEBUG] langsmith.client: Closing Client.session
2025-04-14 17:13:32,746 [DEBUG] httpcore.connection: close.started
2025-04-14 17:13:32,748 [DEBUG] httpcore.connection: close.complete
